# How to frame tokenization as a cool research project?

From a signal processing point of view, tokenization is "statistical quantization over given symbols"

1. Quantization?
  - Yeah as in analog to digital signal processing :wink:
  - Or as in digital to digital, casting float as int.
2. Symbols
  - In the case of language, adopting characters as symbols is too coarse/fine.
  - Go one step back, we use characters as a convention.
    The symbols are the actual compressed/transmitted knowledge
3. Statistical
  Data isnâ€™t scarce.
  Thus, we can't disregard a statistical analysis of the matter.
